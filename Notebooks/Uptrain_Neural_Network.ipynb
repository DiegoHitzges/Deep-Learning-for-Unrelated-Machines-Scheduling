{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fb77c0",
   "metadata": {},
   "source": [
    "# Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47cee93",
   "metadata": {},
   "source": [
    "In this Notebook, we will uptrain our Neural Network on an increased number of Jobs.<br>\n",
    "If this is the first iteration, we will uptrain the Supervised Neural Network to 9 Jobs.<br>\n",
    "If he have uptrained it already before on more than 8 Jobs, we increase the maximum number of Jobs by one and continue training that uptrained version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7bb9aa",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321700a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, LSTM, Concatenate, LeakyReLU, Softmax, Dropout\n",
    "from keras.layers import Lambda, Flatten, Bidirectional, TimeDistributed, Reshape, MultiHeadAttention, LayerNormalization\n",
    "from keras.activations import tanh\n",
    "from keras.models import Model, Sequential\n",
    "import keras.backend\n",
    "import random\n",
    "from copy import copy\n",
    "#import necessary notebooks\n",
    "import import_ipynb\n",
    "from Jobs_and_Machines import *\n",
    "from States_and_Policies import *\n",
    "from Action_Pointer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change working directory\n",
    "work_path = input(\"Working directory to save the Neural Network.\\n\")\n",
    "os.chdir(work_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce12a0",
   "metadata": {},
   "source": [
    "### Create Estimated Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53335e0",
   "metadata": {},
   "source": [
    "Set the increased number of Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c201d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estim = int(input(\"What is the new number of Jobs?\\n\"))\n",
    "\n",
    "n = 8\n",
    "n_min = 3\n",
    "\n",
    "m = 4\n",
    "m_min = 2\n",
    "\n",
    "DS_max = 10\n",
    "DS_min = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(DS_min,DS_max):\n",
    "    \n",
    "    #dictionary will contain data from all respective data sets\n",
    "    data_dict = dict(((n_state,m_state),[[],[]]) \n",
    "                           for n_state in range(n_min,n+1) for m_state in range(m_min,m+1))\n",
    "    \n",
    "    #load data\n",
    "    for n_state in range(n_min, n+1):\n",
    "        for m_state in range(m_min, m+1):\n",
    "            #resource information of the jobs: processing time on every machine and its metadata\n",
    "            x_res = []\n",
    "            #urgency information of the jobs: earliness and weight\n",
    "            x_urg = []\n",
    "            #targets\n",
    "            y = []\n",
    "            #loop over every data set \"DS\"\n",
    "            for DS in [\"0\"*(2-len(str(i)))+str(i) for i in range(DS_min,DS_max+1)]:\n",
    "                training_data_path = f'Data/DataSet_{DS}/LSTM_Data_RR_{DS}/{n_state}-jobs-{m_state}-machines_{DS}.pickle'\n",
    "                with open(training_data_path, 'rb') as f:\n",
    "                    #load data set of key (n_state,m_state)\n",
    "                    data = pickle.load(f)\n",
    "                    #get data about its resources for every job of one state of one Job Scheduling Problem\n",
    "                    jobs_resources = data[0][0][:,:,:-2].reshape((-1,n_state,m_state,4))\n",
    "                    #get data abouts its urgenicies for every job  of one state of one Job Scheduling Problem\n",
    "                    jobs_urgencies = data[0][0][:,:,-2:]\n",
    "                    #add to data of states of other Job Scheduling Problems\n",
    "                    x_res.append(jobs_resources)\n",
    "                    x_urg.append(jobs_urgencies)\n",
    "                    y.append(data[1][0])\n",
    "            #transform list of data from states of Job Scheduling Problems to numpy array and add to final data dictionary\n",
    "            data_dict[(n_state,m_state)][0].append(np.concatenate(x_res))\n",
    "            data_dict[(n_state,m_state)][0].append(np.concatenate(x_urg))\n",
    "            data_dict[(n_state,m_state)][1].append(np.concatenate(y))\n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27669216",
   "metadata": {},
   "source": [
    "We will now load the estimated data and transform it into the form of training data.<br>\n",
    "Note that the path in the function might have to be changed by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114dc727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load estimated data\n",
    "def load_estim_data():\n",
    "    \n",
    "    #dictionary will contain all estimated\n",
    "    estim_data_dict = dict(((n_state,m_state),[[],[]]) \n",
    "                           for n_state in range(n+1, n_estim+1) for m_state in range(m_min,m+1))\n",
    "    \n",
    "    #load estimated data from n+1 to n_estim\n",
    "    for n_state in range(n, n_estim+1):\n",
    "        for m_state in range(m_min, m+1):\n",
    "            #resource information of the jobs: processing time on every machine and its metadata\n",
    "            x_res = []\n",
    "            #urgency information of the jobs: earliness and weight\n",
    "            x_urg = []\n",
    "            #targets\n",
    "            y = []\n",
    "            \"\"\"use your own path here\"\"\"\n",
    "            estim_data_path = f'Data/EstimData/{n_state}_Jobs/LSTM_EstimData_RR/{n_state}-jobs-{m_state}-machines.pickle'\n",
    "            with open(estim_data_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                #get data about its resources for every job of one state of one Job Scheduling Problem\n",
    "                jobs_resources = data[0][0][:,:,:-2].reshape((-1,n_state,m_state,4))\n",
    "                #get data abouts its urgenicies for every job  of one state of one Job Scheduling Problem\n",
    "                jobs_urgencies = data[0][0][:,:,-2:]\n",
    "                #add to data of states of other Job Scheduling Problems\n",
    "                x_res.append(jobs_resources)\n",
    "                x_urg.append(jobs_urgencies)\n",
    "                y.append(data[1][0])\n",
    "\n",
    "            #transform list of data from states of Job Scheduling Problems to numpy array and add to final data dictionary\n",
    "            estim_data_dict[(n_state,m_state)][0].append(np.concatenate(x_res))\n",
    "            estim_data_dict[(n_state,m_state)][0].append(np.concatenate(x_urg))\n",
    "            estim_data_dict[(n_state,m_state)][1].append(np.concatenate(y))\n",
    "\n",
    "    return estim_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f2086",
   "metadata": {},
   "source": [
    "Load the estimated data and bring it into the correct form by splitting it every <i>(n_state,m_state)</i>-subset from it into a list of inputs and a list of targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estim_dict = load_estim_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd36186",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_estim_list, y_estim_list = [], []\n",
    "for key in estim_dict:\n",
    "    \n",
    "    #estimated data\n",
    "    x_estim, y_estim = estim_dict[key]\n",
    "    x_estim_list.append(x_estim)\n",
    "    y_estim_list.append(y_estim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a2c67",
   "metadata": {},
   "source": [
    "Load Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6abb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load desired version of Neural Network\n",
    "def load_NN(NN_name):\n",
    "    \"\"\"path might have to be updated based on users storage\"\"\"\n",
    "    path = 'D:\\\\Job-Scheduling-Files\\'\n",
    "    \n",
    "    NN = keras.models.load_model(f'{path}{NN_name}.h5', custom_objects={'FeedForward': FeedForward, 'Pointer': Pointer, 'MSE_with_Softmax': MSE_with_Softmax, 'costs':costs})\n",
    "    NN.run_eagerly = True\n",
    "    \n",
    "    return NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uptrained_Neural_Network = load_NN(f\"Final_Pointer3\")\n",
    "Uptrained_Neural_Network = load_NN(f\"Uptrained_Final_Pointer_{n_estim-1}_Jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c9d39",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ec2f3",
   "metadata": {},
   "source": [
    "Compile the model the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "Neural_Network.compile(\n",
    "        #custom loss\n",
    "        loss = MSE_with_Softmax,\n",
    "        #optimizer\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "        run_eagerly=True,\n",
    "        #custom metric\n",
    "        metrics = [costs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775a49d",
   "metadata": {},
   "source": [
    "### Uptraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7a5d4",
   "metadata": {},
   "source": [
    "We will now use the estimated data to uptrain the Neural Network.<br>\n",
    "Again, one epoch will consist of the states related to one <i>(n_state,m_state)</i>-subset only.<br>\n",
    "Since is has been trained for any number of Jobs greater than <i>n</i> but lower than <i>n_estim</i> already, we will first train it on the data of <i>n_estim</i> jobs. We loop 5 times through the data, resulting in 5*3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf15af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=0*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=1*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb99993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=2*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d9365",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=3*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f24aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=4*3\n",
    "#load the last three subsets, as these are (n_estim,4), (n_estim,3) and (n_estim,2)\n",
    "for j in range(len(x_estim_list)-3,len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfcaa5",
   "metadata": {},
   "source": [
    "If <i>n_estim > 9</i>, we retrain the Network on the entire estimated data set. One loop therefore consists of 3*i epochs.<br>\n",
    "We do <i>n_estim-n+1</i> loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3 + len(x_estim_list)\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3 + 2*len(x_estim_list)\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3 + 3*len(x_estim_list)\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9feb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We train for {n_estim} Jobs')\n",
    "ep=5*3 + 4*len(x_estim_list)\n",
    "for j in range(len(x_estim_list)):\n",
    "    #train one epoch on  every subset\n",
    "    history = Uptrained_Neural_Network.fit(x_estim_list[j], y_estim_list[j], shuffle=True, batch_size=128, epochs=ep+1, initial_epoch=ep) #validation_data=(x_val_list[j],y_val_list[j])) #callbacks=[my_val_callback]\n",
    "    ep = history.epoch[-1]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff087b",
   "metadata": {},
   "source": [
    "Finally, we save the uptrained version of the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd547450",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uptrained_Neural_Network.save(f'Uptrained_Neural_Network_{n_estim}_Jobs.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
